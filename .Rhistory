# # метод локтя
# fviz_nbclust(df_clust[1:3], kmeans, method = 'wss')
# каменная осыпь
plot(1:(nrow(df_clust)-1), clust.crashes$height, type='b', main='Каменная осыпь')
cl1 <- colMeans(df_clust[clusters == 1, ])
cl2 <- colMeans(df_clust[clusters == 2, ])
cl3 <- colMeans(df_clust[clusters == 3, ])
cl4 <- colMeans(df_clust[clusters == 4, ])
cl5 <- colMeans(df_clust[clusters == 5, ])
df_plot <- data.frame(cl1, cl2, cl3, cl4, cl5)
df_plot1 <- t(df_plot)
df_plot <- t(df_plot1)
barplot(df_plot, col=c('gold','red','green'), ylim=c(0, 0.5))
legend('topleft', colnames(df_clust), fill = c('gold','red','green'))
nrow(df_clust[clusters == 1, ])
nrow(df_clust[clusters == 2, ])
nrow(df_clust[clusters == 3, ])
nrow(df_clust[clusters == 4, ])
nrow(df_clust[clusters == 5, ])
sum(df_deaths[clusters == 1, 'Fatalities'])
sum(df_deaths[clusters == 2, 'Fatalities'])
sum(df_deaths[clusters == 3, 'Fatalities'])
sum(df_deaths[clusters == 4, 'Fatalities'])
sum(df_deaths[clusters == 5, 'Fatalities'])
df_tmp <- df_deaths[-which.max(df_deaths$Fatalities), ]
df_tmp <- df_tmp[-which.max(df_tmp$Fatalities), ]
df_tmp <- df_tmp[-which.max(df_tmp$Fatalities), ]
boxplot(Fatalities~clusters, data=df_tmp, xlab='Clusters',
ylab = 'Fatalities', frame = F, col = rainbow(5))
library (lattice)
df_tmp <- df_deaths[-which.max(df_deaths$Aboard), ]
df_tmp <- df_tmp[-which.max(df_tmp$Aboard), ]
df_tmp <- df_tmp[-which.max(df_tmp$Aboard), ]
xyplot(Fatalities~Aboard, group = clusters, data = df_tmp, auto.key = TRUE, col = rainbow(5))
cloud(Fatalities ~  Aboard* Ground, group = clusters, data = df_deaths,
auto.key = TRUE, col = rainbow(5))
View(df_clust)
gc()
library(cluster)
library(factoextra)
library(ggplot2) # для визуализации данных
library(dplyr) # для манипуляций с данными
df <- read.csv("StudentsPerformance.csv", sep=",", header=T, fileEncoding="UTF-8")
str(df)
summary(df)
ggplot(df, aes(x=gender)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по полу", x="Пол", y="Количество студентов")
ggplot(df, aes(x=race_ethnicity)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по расе/этнической группе", x="Раса/этническая группа", y="Количество студентов")
ggplot(df, aes(x=parental_level_of_education)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по уровню образования родителей", x="Уровень образования", y="Количество студентов")
ggplot(df, aes(x=lunch)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по типу обеда", x="Тип обеда", y="Количество студентов")
ggplot(df, aes(x=test_preparation_course)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по курсу подготовки к экзамену", x="Курс подготовки", y="Количество студентов")
ggplot(df, aes(x=math_score)) +
geom_histogram(fill="#69b3a2", binwidth=5) +
labs(title="Распределение оценок по математике", x="Оценка", y="Количество студентов")
ggplot(df, aes(x=reading_score)) +
geom_histogram(fill="#69b3a2", binwidth=5) +
labs(title="Распределение оценок по чтению", x="Оценка", y="Количество студентов")
ggplot(df, aes(x=writing_score)) +
geom_histogram(fill="#69b3a2", binwidth=5) +
labs(title="Распределение оценок по письму", x="Оценка", y="Количество студентов")
ggplot(df, aes(x=math_score, y=reading_score)) +
geom_point(color="#69b3a2") +
labs(title="Диаграмма рассеяния для оценок по математике и чтению", x="Оценка по математике", y="Оценка по чтению")
# Выбираем переменные для кластеризации
cluster_vars <- c("math_score", "reading_score", "writing_score")
# Создаем матрицу расстояний на основе выбранных переменных
d <- dist(df[, cluster_vars], method = "euclidean")
# Выполняем кластеризацию методом "полного" (complete) сцепления
hc <- hclust(d, method = "complete")
# Строим дендрограмму для визуализации кластеров
plot(hc, cex = 0.6, hang = -1, labels = df$race_ethnicity, main = "Dendrogram of StudentsPerformance data")
# Выбираем переменные для кластеризации
cluster_vars <- c("math_score", "reading_score", "writing_score")
# Создаем матрицу расстояний на основе выбранных переменных
d <- dist(df[, cluster_vars], method = "euclidean")
# Выполняем кластеризацию методом "полного" (complete) сцепления
hc <- hclust(d, method = "complete")
plot(hc, labels=F)
rect.hclust(hc,k = 5, border="red")
# Строим график для метода локтя
# метод локтя
fviz_nbclust(df[6:8], kmeans, method = 'wss')
plot(1:(nrow(df)-1), hc$height, type='b', main='Каменная осыпь')
df$average_score <- (df$math_score + df$reading_score + df$writing_score)/3
library(ggplot2)
ggplot(df, aes(x = parental_level_of_education, y = average_score)) +
geom_boxplot(fill = "lightblue") +
labs(title = "Зависимость между образованием родителей и успеваемостью студентов",
x = "Образование родителей", y = "Средняя оценка")
library (lattice)
clusters <- cutree(hc, k = 5)
xyplot(math_score~reading_score, group = clusters, data = df, auto.key = TRUE, col = rainbow(5))
cloud(math_score~reading_score*writing_score, group = clusters,  data = df,
auto.key = TRUE, col = rainbow(5))
library(klaR)
ind <- sample(2, nrow(hc), replace=TRUE, prob=c(0.7, 0.3))
library(klaR)
ind <- sample(2, nrow(df), replace=TRUE, prob=c(0.7, 0.3))
hc_train <- data.frame(hc, clusters)
library(klaR)
ind <- sample(2, nrow(df), replace=TRUE, prob=c(0.7, 0.3))
hc_train <- data.frame(df, clusters)
hc_train <- hc_train[ind == 1, ]
hc_train$clusters <- as.factor(hc_train$clusters)
hc_test <- hc[ind == 2, ]
library(cluster)
library(factoextra)
library(ggplot2) # для визуализации данных
library(dplyr) # для манипуляций с данными
df <- read.csv("StudentsPerformance.csv", sep=",", header=T, fileEncoding="UTF-8")
str(df)
summary(df)
ggplot(df, aes(x=gender)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по полу", x="Пол", y="Количество студентов")
ggplot(df, aes(x=race_ethnicity)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по расе/этнической группе", x="Раса/этническая группа", y="Количество студентов")
ggplot(df, aes(x=parental_level_of_education)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по уровню образования родителей", x="Уровень образования", y="Количество студентов")
ggplot(df, aes(x=lunch)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по типу обеда", x="Тип обеда", y="Количество студентов")
ggplot(df, aes(x=test_preparation_course)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по курсу подготовки к экзамену", x="Курс подготовки", y="Количество студентов")
ggplot(df, aes(x=math_score)) +
geom_histogram(fill="#69b3a2", binwidth=5) +
labs(title="Распределение оценок по математике", x="Оценка", y="Количество студентов")
ggplot(df, aes(x=reading_score)) +
geom_histogram(fill="#69b3a2", binwidth=5) +
labs(title="Распределение оценок по чтению", x="Оценка", y="Количество студентов")
ggplot(df, aes(x=writing_score)) +
geom_histogram(fill="#69b3a2", binwidth=5) +
labs(title="Распределение оценок по письму", x="Оценка", y="Количество студентов")
ggplot(df, aes(x=math_score, y=reading_score)) +
geom_point(color="#69b3a2") +
labs(title="Диаграмма рассеяния для оценок по математике и чтению", x="Оценка по математике", y="Оценка по чтению")
# Выбираем переменные для кластеризации
cluster_vars <- c("math_score", "reading_score", "writing_score")
# Создаем матрицу расстояний на основе выбранных переменных
d <- dist(df[, cluster_vars], method = "euclidean")
# Выполняем кластеризацию методом "полного" (complete) сцепления
hc <- hclust(d, method = "complete")
# Строим дендрограмму для визуализации кластеров
plot(hc, cex = 0.6, hang = -1, labels = df$race_ethnicity, main = "Dendrogram of StudentsPerformance data")
# Выбираем переменные для кластеризации
cluster_vars <- c("math_score", "reading_score", "writing_score")
# Создаем матрицу расстояний на основе выбранных переменных
d <- dist(df[, cluster_vars], method = "euclidean")
# Выполняем кластеризацию методом "полного" (complete) сцепления
hc <- hclust(d, method = "complete")
plot(hc, labels=F)
rect.hclust(hc,k = 5, border="red")
# Строим график для метода локтя
# метод локтя
fviz_nbclust(df[6:8], kmeans, method = 'wss')
plot(1:(nrow(df)-1), hc$height, type='b', main='Каменная осыпь')
df$average_score <- (df$math_score + df$reading_score + df$writing_score)/3
library(ggplot2)
ggplot(df, aes(x = parental_level_of_education, y = average_score)) +
geom_boxplot(fill = "lightblue") +
labs(title = "Зависимость между образованием родителей и успеваемостью студентов",
x = "Образование родителей", y = "Средняя оценка")
library (lattice)
clusters <- cutree(hc, k = 5)
xyplot(math_score~reading_score, group = clusters, data = df, auto.key = TRUE, col = rainbow(5))
cloud(math_score~reading_score*writing_score, group = clusters,  data = df,
auto.key = TRUE, col = rainbow(5))
library(klaR)
ind <- sample(2, nrow(df), replace=TRUE, prob=c(0.7, 0.3))
hc_train <- data.frame(hc, clusters)
hc_train <- data.frame(df[6:8], clusters)
library(klaR)
ind <- sample(2, nrow(df), replace=TRUE, prob=c(0.7, 0.3))
hc_train <- data.frame(df[6:8], clusters)
hc_train <- hc_train[ind == 1, ]
hc_train$clusters <- as.factor(hc_train$clusters)
hc_test <- hc[ind == 2, ]
library(klaR)
ind <- sample(2, nrow(df), replace=TRUE, prob=c(0.7, 0.3))
df_train <- data.frame(df[6:8], clusters)
df_train <- df_train[ind == 1, ]
df_train$clusters <- as.factor(df_train$clusters)
df_test <- df[ind == 2, ]
clusters_test <- clusters[ind == 2]
df_test <- data.frame(df_test, clusters=clusters_test)
naive_crashes <- NaiveBayes(clusters~., data=df_train)
library(klaR)
ind <- sample(2, nrow(df), replace=TRUE, prob=c(0.7, 0.3))
df_train <- data.frame(df[6:8], clusters)
df_train <- df_train[ind == 1, ]
df_train$clusters <- as.factor(df_train$clusters)
df_test <- df[ind == 2, ]
clusters_test <- clusters[ind == 2]
df_test <- data.frame(df_test, clusters=clusters_test)
naive_crashes <- NaiveBayes(clusters~., data=df_train)
library(klaR)
ind <- sample(2, nrow(df), replace=TRUE, prob=c(0.7, 0.3))
df_train <- data.frame(df[6:8], clusters)
df_train <- df_train[ind == 1, ]
df_train$clusters <- as.factor(df_train$clusters)
df_test <- df[ind == 2, ]
clusters_test <- clusters[ind == 2]
df_test <- data.frame(df_test, clusters=clusters_test)
naive_crashes <- NaiveBayes(clusters~., data=df_train)
naive_crashes$tables
par(mfrow=c(3,1))
plot(naive_crashes,lwd = 2, legendplot=FALSE)
pred_b <- predict(naive_crashes, df_train[, -4])$class
table(Факт = df_train$clusters, Прогноз = pred_b)
acc_b <- mean(pred_b == df_train$clusters)
paste0("Точность = ", round(100*acc_b, 2), "%")
library(party)
myFormula <- clusters ~ .
crashes_ctree <- ctree(myFormula, data=df_train)
pred_t <- predict(crashes_ctree, df_train[, -4])
table(Факт = df_train$clusters, Прогноз = pred_t)
acc_t <- mean(pred_t == df_train$clusters)
paste0("Точность = ", round(100*acc_t, 2), "%")
dev.off()
plot(crashes_ctree)
library(randomForest)
rf <- randomForest(clusters ~ ., data=df_train, ntree=100, proximity=TRUE)
pred_rf <- predict(crashes_ctree, df_train[, -4])
table(Факт = df_train$clusters, Прогноз = pred_rf)
acc_rf <- mean(pred_rf == df_train$clusters)
paste0("Точность = ", round(100*acc_rf, 2), "%")
library(klaR)
ind <- sample(2, nrow(df), replace=TRUE, prob=c(0.7, 0.3))
df_train <- data.frame(df[6:8], clusters)
df_train <- df_train[ind == 1, ]
df_train$clusters <- as.factor(df_train$clusters)
df_test <- df[ind == 2, ]
clusters_test <- clusters[ind == 2]
df_test <- data.frame(df_test, clusters=clusters_test)
naive_crashes <- NaiveBayes(clusters~., data=df_train)
naive_crashes$tables
par(mfrow=c(3,1))
plot(naive_crashes,lwd = 2, legendplot=FALSE)
pred_b <- predict(naive_crashes, df_train[, -4])$class
table(Факт = df_train$clusters, Прогноз = pred_b)
acc_b <- mean(pred_b == df_train$clusters)
paste0("Точность = ", round(100*acc_b, 2), "%")
library(party)
myFormula <- clusters ~ .
crashes_ctree <- ctree(myFormula, data=df_train)
pred_t <- predict(crashes_ctree, df_train[, -4])
table(Факт = df_train$clusters, Прогноз = pred_t)
acc_t <- mean(pred_t == df_train$clusters)
paste0("Точность = ", round(100*acc_t, 2), "%")
dev.off()
plot(crashes_ctree)
library(randomForest)
rf <- randomForest(clusters ~ ., data=df_train, ntree=100, proximity=TRUE)
pred_rf <- predict(crashes_ctree, df_train[, -4])
table(Факт = df_train$clusters, Прогноз = pred_rf)
acc_rf <- mean(pred_rf == df_train$clusters)
paste0("Точность = ", round(100*acc_rf, 2), "%")
library(cluster)
library(factoextra)
library(ggplot2) # для визуализации данных
library(dplyr) # для манипуляций с данными
df <- read.csv("StudentsPerformance.csv", sep=",", header=T, fileEncoding="UTF-8")
str(df)
summary(df)
ggplot(df, aes(x=gender)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по полу", x="Пол", y="Количество студентов")
ggplot(df, aes(x=race_ethnicity)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по расе/этнической группе", x="Раса/этническая группа", y="Количество студентов")
ggplot(df, aes(x=parental_level_of_education)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по уровню образования родителей", x="Уровень образования", y="Количество студентов")
ggplot(df, aes(x=lunch)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по типу обеда", x="Тип обеда", y="Количество студентов")
ggplot(df, aes(x=test_preparation_course)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по курсу подготовки к экзамену", x="Курс подготовки", y="Количество студентов")
ggplot(df, aes(x=math_score)) +
geom_histogram(fill="#69b3a2", binwidth=5) +
labs(title="Распределение оценок по математике", x="Оценка", y="Количество студентов")
ggplot(df, aes(x=reading_score)) +
geom_histogram(fill="#69b3a2", binwidth=5) +
labs(title="Распределение оценок по чтению", x="Оценка", y="Количество студентов")
ggplot(df, aes(x=writing_score)) +
geom_histogram(fill="#69b3a2", binwidth=5) +
labs(title="Распределение оценок по письму", x="Оценка", y="Количество студентов")
ggplot(df, aes(x=math_score, y=reading_score)) +
geom_point(color="#69b3a2") +
labs(title="Диаграмма рассеяния для оценок по математике и чтению", x="Оценка по математике", y="Оценка по чтению")
# Выбираем переменные для кластеризации
cluster_vars <- c("math_score", "reading_score", "writing_score")
# Создаем матрицу расстояний на основе выбранных переменных
d <- dist(df[, cluster_vars], method = "euclidean")
# Выполняем кластеризацию методом "полного" (complete) сцепления
hc <- hclust(d, method = "complete")
# Строим дендрограмму для визуализации кластеров
plot(hc, cex = 0.6, hang = -1, labels = df$race_ethnicity, main = "Dendrogram of StudentsPerformance data")
# Выбираем переменные для кластеризации
cluster_vars <- c("math_score", "reading_score", "writing_score")
# Создаем матрицу расстояний на основе выбранных переменных
d <- dist(df[, cluster_vars], method = "euclidean")
# Выполняем кластеризацию методом "полного" (complete) сцепления
hc <- hclust(d, method = "complete")
plot(hc, labels=F)
rect.hclust(hc,k = 5, border="red")
# Строим график для метода локтя
# метод локтя
fviz_nbclust(df[6:8], kmeans, method = 'wss')
plot(1:(nrow(df)-1), hc$height, type='b', main='Каменная осыпь')
df$average_score <- (df$math_score + df$reading_score + df$writing_score)/3
library(ggplot2)
ggplot(df, aes(x = parental_level_of_education, y = average_score)) +
geom_boxplot(fill = "lightblue") +
labs(title = "Зависимость между образованием родителей и успеваемостью студентов",
x = "Образование родителей", y = "Средняя оценка")
library (lattice)
clusters <- cutree(hc, k = 5)
xyplot(math_score~reading_score, group = clusters, data = df, auto.key = TRUE, col = rainbow(5))
cloud(math_score~reading_score*writing_score, group = clusters,  data = df,
auto.key = TRUE, col = rainbow(5))
ggplot(df, aes(x=parental_level_of_education, y=(math_score+reading_score+writing_score)/3)) +
geom_point() +
labs(x="Образование родителей", y="Средняя оценка по всем предметам") +
theme_bw()
library(klaR)
ind <- sample(2, nrow(df), replace=TRUE, prob=c(0.7, 0.3))
df_train <- data.frame(df[6:8], clusters)
df_train <- df_train[ind == 1, ]
df_train$clusters <- as.factor(df_train$clusters)
df_test <- df[ind == 2, ]
clusters_test <- clusters[ind == 2]
df_test <- data.frame(df_test, clusters=clusters_test)
naive_crashes <- NaiveBayes(clusters~., data=df_train)
naive_crashes$tables
par(mfrow=c(3,1))
plot(naive_crashes,lwd = 2, legendplot=FALSE)
pred_b <- predict(naive_crashes, df_train[, -4])$class
table(Факт = df_train$clusters, Прогноз = pred_b)
acc_b <- mean(pred_b == df_train$clusters)
paste0("Точность = ", round(100*acc_b, 2), "%")
library(party)
myFormula <- clusters ~ .
crashes_ctree <- ctree(myFormula, data=df_train)
pred_t <- predict(crashes_ctree, df_train[, -4])
table(Факт = df_train$clusters, Прогноз = pred_t)
acc_t <- mean(pred_t == df_train$clusters)
paste0("Точность = ", round(100*acc_t, 2), "%")
dev.off()
plot(crashes_ctree)
library(randomForest)
rf <- randomForest(clusters ~ ., data=df_train, ntree=100, proximity=TRUE)
pred_rf <- predict(crashes_ctree, df_train[, -4])
table(Факт = df_train$clusters, Прогноз = pred_rf)
acc_rf <- mean(pred_rf == df_train$clusters)
paste0("Точность = ", round(100*acc_rf, 2), "%")
library(klaR)
ind <- sample(2, nrow(df), replace=TRUE, prob=c(0.7, 0.3))
df_train <- data.frame(df[6:8], clusters)
df_train <- df_train[ind == 1, ]
df_train$clusters <- as.factor(df_train$clusters)
df_test <- df[ind == 2, ]
clusters_test <- clusters[ind == 2]
df_test <- data.frame(df_test, clusters=clusters_test)
naive_crashes <- NaiveBayes(clusters~., data=df_train)
naive_crashes$tables
par(mfrow=c(3,1))
plot(naive_crashes,lwd = 2, legendplot=FALSE)
pred_b <- predict(naive_crashes, df_train[, -4])$class
table(Факт = df_train$clusters, Прогноз = pred_b)
acc_b <- mean(pred_b == df_train$clusters)
paste0("Точность = ", round(100*acc_b, 2), "%")
library(party)
myFormula <- clusters ~ .
crashes_ctree <- ctree(myFormula, data=df_train)
pred_t <- predict(crashes_ctree, df_train[, -4])
table(Факт = df_train$clusters, Прогноз = pred_t)
acc_t <- mean(pred_t == df_train$clusters)
paste0("Точность = ", round(100*acc_t, 2), "%")
dev.off()
plot(crashes_ctree)
acc_t <- mean(pred_t == df_train$clusters)
paste0("Точность = ", round(100*acc_t, 2), "%")
dev.off()
plot(crashes_ctree)
library(randomForest)
rf <- randomForest(clusters ~ ., data=df_train, ntree=100, proximity=TRUE)
pred_rf <- predict(crashes_ctree, df_train[, -4])
table(Факт = df_train$clusters, Прогноз = pred_rf)
acc_rf <- mean(pred_rf == df_train$clusters)
paste0("Точность = ", round(100*acc_rf, 2), "%")
pred_t <- predict(crashes_ctree, df_test[, -4])
table(Факт = df_test$clusters, Прогноз = pred_t)
acc_t <- mean(pred_t == df_test$clusters)
paste0("Точность = ", round(100*acc_t, 2), "%")
dev.off()
plot(crashes_ctree)
library(cluster)
library(factoextra)
library(ggplot2) # для визуализации данных
library(dplyr) # для манипуляций с данными
df <- read.csv("StudentsPerformance.csv", sep=",", header=T, fileEncoding="UTF-8")
str(df)
summary(df)
ggplot(df, aes(x=gender)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по полу", x="Пол", y="Количество студентов")
ggplot(df, aes(x=race_ethnicity)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по расе/этнической группе", x="Раса/этническая группа", y="Количество студентов")
ggplot(df, aes(x=parental_level_of_education)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по уровню образования родителей", x="Уровень образования", y="Количество студентов")
ggplot(df, aes(x=lunch)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по типу обеда", x="Тип обеда", y="Количество студентов")
ggplot(df, aes(x=test_preparation_course)) +
geom_bar(fill="#69b3a2") +
labs(title="Распределение по курсу подготовки к экзамену", x="Курс подготовки", y="Количество студентов")
ggplot(df, aes(x=math_score)) +
geom_histogram(fill="#69b3a2", binwidth=5) +
labs(title="Распределение оценок по математике", x="Оценка", y="Количество студентов")
ggplot(df, aes(x=reading_score)) +
geom_histogram(fill="#69b3a2", binwidth=5) +
labs(title="Распределение оценок по чтению", x="Оценка", y="Количество студентов")
ggplot(df, aes(x=writing_score)) +
geom_histogram(fill="#69b3a2", binwidth=5) +
labs(title="Распределение оценок по письму", x="Оценка", y="Количество студентов")
ggplot(df, aes(x=math_score, y=reading_score)) +
geom_point(color="#69b3a2") +
labs(title="Диаграмма рассеяния для оценок по математике и чтению", x="Оценка по математике", y="Оценка по чтению")
ggplot(df, aes(x=parental_level_of_education, y=(math_score+reading_score+writing_score)/3)) +
geom_point() +
labs(x="Образование родителей", y="Средняя оценка по всем предметам") +
theme_bw()
# Выбираем переменные для кластеризации
cluster_vars <- c("math_score", "reading_score", "writing_score")
# Создаем матрицу расстояний на основе выбранных переменных
d <- dist(df[, cluster_vars], method = "euclidean")
# Выполняем кластеризацию методом "полного" (complete) сцепления
hc <- hclust(d, method = "complete")
# Строим дендрограмму для визуализации кластеров
plot(hc, cex = 0.6, hang = -1, labels = df$race_ethnicity, main = "Dendrogram of StudentsPerformance data")
# Выбираем переменные для кластеризации
cluster_vars <- c("math_score", "reading_score", "writing_score")
# Создаем матрицу расстояний на основе выбранных переменных
d <- dist(df[, cluster_vars], method = "euclidean")
# Выполняем кластеризацию методом "полного" (complete) сцепления
hc <- hclust(d, method = "complete")
plot(hc, labels=F)
rect.hclust(hc,k = 5, border="red")
# Строим график для метода локтя
# метод локтя
fviz_nbclust(df[6:8], kmeans, method = 'wss')
plot(1:(nrow(df)-1), hc$height, type='b', main='Каменная осыпь')
df$average_score <- (df$math_score + df$reading_score + df$writing_score)/3
library(ggplot2)
ggplot(df, aes(x = parental_level_of_education, y = average_score)) +
geom_boxplot(fill = "lightblue") +
labs(title = "Зависимость между образованием родителей и успеваемостью студентов",
x = "Образование родителей", y = "Средняя оценка")
library (lattice)
clusters <- cutree(hc, k = 5)
xyplot(math_score~reading_score, group = clusters, data = df, auto.key = TRUE, col = rainbow(5))
cloud(math_score~reading_score*writing_score, group = clusters,  data = df,
auto.key = TRUE, col = rainbow(5))
View(df)
View(df)
library(klaR)
ind <- sample(2, nrow(df), replace=TRUE, prob=c(0.7, 0.3))
df_train <- data.frame(df[6:8], clusters)
df_train <- df_train[ind == 1, ]
df_train$clusters <- as.factor(df_train$clusters)
df_test <- df[ind == 2, ]
clusters_test <- clusters[ind == 2]
df_test <- data.frame(df_test, clusters=clusters_test)
naive_crashes <- NaiveBayes(clusters~., data=df_train)
naive_crashes$tables
par(mfrow=c(3,1))
plot(naive_crashes,lwd = 2, legendplot=FALSE)
pred_b <- predict(naive_crashes, df_train[, -4])$class
table(Факт = df_train$clusters, Прогноз = pred_b)
acc_b <- mean(pred_b == df_train$clusters)
paste0("Точность = ", round(100*acc_b, 2), "%")
library(party)
myFormula <- clusters ~ .
crashes_ctree <- ctree(myFormula, data=df_train)
pred_t <- predict(crashes_ctree, df_test[, -4])
table(Факт = df_test$clusters, Прогноз = pred_t)
acc_t <- mean(pred_t == df_test$clusters)
paste0("Точность = ", round(100*acc_t, 2), "%")
dev.off()
plot(crashes_ctree)
library(randomForest)
rf <- randomForest(clusters ~ ., data=df_train, ntree=100, proximity=TRUE)
pred_rf <- predict(crashes_ctree, df_train[, -4])
table(Факт = df_train$clusters, Прогноз = pred_rf)
acc_rf <- mean(pred_rf == df_train$clusters)
paste0("Точность = ", round(100*acc_rf, 2), "%")
